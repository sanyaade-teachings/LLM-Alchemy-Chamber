<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Mastering Applied AI, One Concept at a Time"><meta name=author content="Adithya S Kolavi"><link href=https://aiengineering.academy/LLM/TheoryBehindFinetuning/ORPO/ rel=canonical><link href=../DPO/ rel=prev><link href=../GRPO/ rel=next><link rel=icon href=../../../assets/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.7"><title>ORPO(Odds Ratio Preference Optimization) - AI Engineering Academy</title><link rel=stylesheet href=../../../assets/stylesheets/main.8608ea7d.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JP3605WT7D"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JP3605WT7D",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JP3605WT7D",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="ORPO(Odds Ratio Preference Optimization) - AI Engineering Academy"><meta property=og:description content="Mastering Applied AI, One Concept at a Time"><meta property=og:image content=https://aiengineering.academy/assets/images/social/LLM/TheoryBehindFinetuning/ORPO.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://aiengineering.academy/LLM/TheoryBehindFinetuning/ORPO/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="ORPO(Odds Ratio Preference Optimization) - AI Engineering Academy"><meta name=twitter:description content="Mastering Applied AI, One Concept at a Time"><meta name=twitter:image content=https://aiengineering.academy/assets/images/social/LLM/TheoryBehindFinetuning/ORPO.png><link rel=stylesheet href=../../../assets/stylesheets/custom.7c86dd97.min.css><!-- PostHog Analytics --><script>
  !(function (t, e) {
    var o, n, p, r;
    e.__SV ||
      ((window.posthog = e),
      (e._i = []),
      (e.init = function (i, s, a) {
        function g(t, e) {
          var o = e.split(".");
          2 == o.length && ((t = t[o[0]]), (e = o[1])),
            (t[e] = function () {
              t.push([e].concat(Array.prototype.slice.call(arguments, 0)));
            });
        }
        ((p = t.createElement("script")).type = "text/javascript"),
          (p.async = !0),
          (p.src = s.api_host + "/static/array.js"),
          (r = t.getElementsByTagName("script")[0]).parentNode.insertBefore(
            p,
            r
          );
        var u = e;
        for (
          void 0 !== a ? (u = e[a] = []) : (a = "posthog"),
            u.people = u.people || [],
            u.toString = function (t) {
              var e = "posthog";
              return (
                "posthog" !== a && (e += "." + a), t || (e += " (stub)"), e
              );
            },
            u.people.toString = function () {
              return u.toString(1) + ".people (stub)";
            },
            o =
              "capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset isFeatureEnabled onFeatureFlags getFeatureFlag getFeatureFlagPayload reloadFeatureFlags group updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures getActiveMatchingSurveys getSurveys onSessionId".split(
                " "
              ),
            n = 0;
          n < o.length;
          n++
        )
          g(u, o[n]);
        e._i.push([i, s, a]);
      }),
      (e.__SV = 1));
  })(document, window.posthog || []);
  posthog.init("YOUR_POSTHOG_KEY", { api_host: "https://app.posthog.com" });
</script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@adithya_s_k</strong> on <a href=https://x.com/adithya_s_k> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Engineering Academy" class="md-header__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Engineering Academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> ORPO(Odds Ratio Preference Optimization) </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../PromptEngineering/ class=md-tabs__link> Prompt Engineering </a> </li> <li class=md-tabs__item> <a href=../../../RAG/ class=md-tabs__link> RAG </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> LLM </a> </li> <li class=md-tabs__item> <a href=../../../Deployment/ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../../Agents/ class=md-tabs__link> Agents </a> </li> <li class=md-tabs__item> <a href=../../../Projects/ class=md-tabs__link> Projects </a> </li> <li class=md-tabs__item> <a href=../../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Engineering Academy" class="md-nav__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> AI Engineering Academy </label> <div class=md-nav__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../PromptEngineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../RAG/ class=md-nav__link> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> LLM </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2 checked> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex> <span class=md-ellipsis> Finetuning Techniques </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=true> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Finetuning Techniques </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../PreTrain/ class=md-nav__link> <span class=md-ellipsis> PreTraining LLMs </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../SFT/ class=md-nav__link> <span class=md-ellipsis> SFT </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href=../PPO/ class=md-nav__link> <span class=md-ellipsis> PPO(Proximal Policy Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../DPO/ class=md-nav__link> <span class=md-ellipsis> DPO(Direct Preference Optimization) </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> ORPO(Odds Ratio Preference Optimization) </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../GRPO/ class=md-nav__link> <span class=md-ellipsis> GRPO(Group Relative Policy Optimization) </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> LLM Finetuning Hands on </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> LLM Finetuning Hands on </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Gemma/ class=md-nav__link> <span class=md-ellipsis> Gemma </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../LLama2/Llama2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Llama2 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Llama3_finetuning_notebook.ipynb class=md-nav__link> <span class=md-ellipsis> Llama3 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Mistral-7b/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> VLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> VLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../VLM/Florence2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Florence2 </span> </a> </li> <li class=md-nav__item> <a href=../../VLM/PaliGemma_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> PaliGemma </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_5> <div class="md-nav__link md-nav__container"> <a href=../../LLMArchitecture/ParameterCount/ class="md-nav__link "> <span class=md-ellipsis> LLM Architecture </span> </a> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> LLM Architecture </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Projects/ class=md-nav__link> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/edit/master/docs/LLM/TheoryBehindFinetuning/ORPO.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/raw/master/docs/LLM/TheoryBehindFinetuning/ORPO.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1>ORPO(Odds Ratio Preference Optimization)</h1> <div><p><strong>What is ORPO?</strong></p> <p><strong>ORPO</strong>, or Odds Ratio Preference Optimization, is a method to fine-tune large language models (LLMs) like GPT-3 or Llama-2, making them generate responses humans prefer, such as accurate and helpful answers. Imagine asking, “What’s the capital of France?” and the model gives “Paris” (preferred) or “London” (not preferred). ORPO helps the model learn to choose “Paris” more often by using a math trick called the odds ratio during the training process.</p> <p><strong>How Does It Work?</strong></p> <p>ORPO builds on supervised fine-tuning (SFT), where you train the model on examples of prompts and preferred responses. But it adds a step: for each prompt, it also looks at a dispreferred response and uses the odds ratio to compare them. The odds ratio measures how much more likely the model is to generate the preferred response versus the dispreferred one. It then adjusts the model to favor the preferred response, all in one go, without needing extra steps like training a separate reward model.</p> <p><strong>Why It’s Beneficial</strong></p> <p>What’s great is ORPO makes the process simpler and cheaper than traditional methods like Reinforcement Learning from Human Feedback (RLHF), which needs multiple steps and more computer power. It’s like getting the same result with fewer steps, saving time and resources, which is a big deal for making AI more accessible.</p> <hr> <hr> <p><strong>Comprehensive Analysis of Odds Ratio Preference Optimization (ORPO)</strong></p> <p><strong>Introduction to ORPO and Its Relevance in LLM Fine-Tuning</strong></p> <p>Odds Ratio Preference Optimization (ORPO) is a novel method for fine-tuning large language models (LLMs) to align them with human preferences, introduced in the paper "ORPO: Monolithic Preference Optimization without Reference Model" (<a href=https://arxiv.org/abs/2403.07691>ORPO: Monolithic Preference Optimization without Reference Model</a>). LLMs, such as GPT-3, BERT, Llama-2, and Mistral, are neural networks trained on vast text corpora to understand and generate human language, typically using self-supervision like next-word prediction during pre-training. However, aligning these models with human preferences for practical applications, such as chatbots, content generation, and decision support systems, requires additional fine-tuning. Traditional methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have been used, but ORPO offers a streamlined approach by integrating preference alignment directly into Supervised Fine-Tuning (SFT) using the odds ratio, reducing complexity and computational costs.</p> <p>The thinking trace initially explored what ORPO could stand for, considering possibilities like Online Reasoning and Problem-solving or Optimized Reinforcement Policy Optimization, but confirmed it as Odds Ratio Preference Optimization based on search results, particularly the arXiv paper and related blog posts. This confirmation aligns with the observed trend in AI research towards efficient alignment methods, making ORPO a timely topic for a detailed analysis, catering to both beginners and those seeking technical depth.</p> <p><strong>Background: Challenges in LLM Alignment and Traditional Methods</strong></p> <p>Aligning LLMs with human preferences is crucial for ensuring their outputs are helpful, truthful, and aligned with societal norms. The thinking trace outlines traditional methods, starting with Supervised Fine-Tuning (SFT), where the model is trained on a dataset of prompts and desired responses to mimic specific outputs. However, SFT alone may not suffice for achieving the desired level of alignment, especially for complex tasks requiring nuanced preferences.</p> <p>Reinforcement Learning from Human Feedback (RLHF), detailed in "Fine-Tuning Language Models from Human Preferences" by OpenAI (<a href=https://www.deepmind.com/publications/fine-tuning-language-models-from-human-preferences>Fine-Tuning Language Models from Human Preferences</a>), involves a multi-step process:</p> <ol> <li>Generate responses with the pre-trained model for a set of prompts.</li> <li>Collect human feedback to rank these responses, indicating which are better.</li> <li>Train a separate reward model using supervised learning based on the rankings.</li> <li>Fine-tune the LLM using reinforcement learning, such as Proximal Policy Optimization (PPO), to maximize the expected reward from the reward model.</li> </ol> <p>This process, as noted in the thinking trace, is resource-intensive and computationally expensive due to the need for an additional reward model and the instability of reinforcement learning, particularly in large models.</p> <p>Direct Preference Optimization (DPO), introduced in "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" by Anthropic (<a href=https://www.deepmind.com/publications/direct-preference-optimization-your-language-model-is-secretly-a-reward-model>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>), simplifies RLHF by directly optimizing the model based on pairwise preference data without a separate reward model. DPO uses a loss function that compares the log probabilities of preferred and dispreferred responses, as seen in its implementation details.</p> <p>ORPO builds on these ideas but introduces a novel approach by integrating preference alignment into SFT using the odds ratio, eliminating the need for an additional preference alignment phase, as highlighted in the thinking trace’s exploration of its reference model-free, monolithic nature.</p> <p><strong>Definition and Context of ORPO</strong></p> <p>ORPO is defined as a fine-tuning technique that modifies the SFT process to incorporate human preference data using the odds ratio, a statistical measure that contrasts the likelihoods of generating preferred versus dispreferred responses. The thinking trace clarifies that ORPO requires a dataset where each prompt</p> <p><span class=arithmatex>\(x\)</span> is associated with a preferred response <span class=arithmatex>\(yw\)</span> and a dispreferred response <span class=arithmatex>\(yl\)</span> , similar to DPO, but uses the odds ratio in its loss function, offering a different way to contrast these responses.</p> <p>The significance of ORPO, as noted in blog posts like "Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning" (<a href=https://attentions.ai/blog/demystifying-orpo-a-revolutionary-paradigm-in-language-model-fine-tuning/ >Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning</a>) and "ORPO, A New Era for LLMs?" (<a href=https://medium.com/@ignacio.de.gregorio.noblejas/orpo-a-new-era-for-llms-31f99acafec5>ORPO, A New Era for LLMs?</a>), lies in its potential to reduce training costs and complexity, making LLM deployment more accessible for open-source and enterprise communities. This aligns with the thinking trace’s observation of ORPO’s democratizing force, as mentioned in the Medium article.</p> <p><strong>Mathematical Formulation of ORPO</strong></p> <p>The mathematical foundation of ORPO is central to its operation, and the thinking trace provides a detailed derivation based on the browse_page result from the arXiv paper. The loss function consists of two components:</p> <ol> <li><strong>Supervised Fine-Tuning Loss (LSFT)</strong>: This is the standard negative log-likelihood loss for generating the preferred response, defined as:where <span class=arithmatex>\(P(yw∣x)\)</span> is the probability of generating the preferred response given <span class=arithmatex>\(yw\)</span> the prompt , computed as the product of token probabilities in the sequence.</li> </ol> <p><span class=arithmatex>\(LSFT=−log⁡P(yw∣x)\)</span></p> <p><span class=arithmatex>\(x\)</span></p> <ol> <li><strong>Odds Ratio Loss (LOR)</strong>: This loss incorporates the odds ratio to contrast preferred and dispreferred responses, defined as:where is the sigmoid function, , is the preferred response, and is the dispreferred response. The odds ratio is:The thinking trace initially struggled with this, noting that for sequence probabilities in LLMs, is typically very small, making , so the odds ratio approximates to , and the log odds ratio to , similar to DPO. However, the paper’s use of odds suggests a nuanced difference, potentially at the token level or in how probabilities are interpreted.</li> </ol> <p><span class=arithmatex>\(LOR=−log⁡σ(log⁡(odds(yw∣x)odds(yl∣x)))\)</span></p> <p><span class=arithmatex>\(σ\)</span></p> <p><span class=arithmatex>\(odds(y∣x)=P(y∣x)1−P(y∣x)\)</span></p> <p><span class=arithmatex>\(yw\)</span></p> <p><span class=arithmatex>\(yl\)</span></p> <p><span class=arithmatex>\(odds(yw∣x)odds(yl∣x)=P(yw∣x)/(1−P(yw∣x))P(yl∣x)/(1−P(yl∣x))=P(yw∣x)P(yl∣x)⋅1−P(yl∣x)1−P(yw∣x)\)</span></p> <p><span class=arithmatex>\(P(y∣x)\)</span></p> <p><span class=arithmatex>\(1−P(y∣x)≈1\)</span></p> <p><span class=arithmatex>\(P(yw∣x)P(yl∣x)\)</span></p> <p><span class=arithmatex>\(log⁡P(yw∣x)−log⁡P(yl∣x)\)</span></p> <p>The overall loss function is:</p> <p><span class=arithmatex>\(LORPO=E(x,yw,yl)[LSFT+λ⋅LOR]\)</span></p> <p>where</p> <p><span class=arithmatex>\(λ\)</span></p> <p>is a weighting factor, and the expectation is over the dataset of prompt-preferred-dispreferred triples. The gradient of</p> <p><span class=arithmatex>\(LOR\)</span></p> <p>is given by:</p> <p><span class=arithmatex>\(∇θLOR=δ(d)⋅h(d)\)</span></p> <p>where</p> <p><span class=arithmatex>\(δ(d)=1+(odds(yw∣x)odds(yl∣x))−1\)</span></p> <p>and</p> <p><span class=arithmatex>\(h(d)=−∇θlog⁡P(yw∣x)1−P(yw∣x)+∇θlog⁡P(yl∣x)1−P(yl∣x)\)</span></p> <p>, for <span class=arithmatex>\(d=(x,yw,yl)∼D\)</span>, as detailed in the browse_page result. This formulation, while complex, ensures the model adjusts to favor preferred responses, as noted in the thinking trace’s exploration.</p> <p><strong>Intuition Behind ORPO’s Operation</strong></p> <p>The intuition behind ORPO, as explained in the thinking trace, is to directly incorporate preference information into SFT by comparing the odds of generating preferred versus dispreferred responses. This is analogous to ranking problems, where a model learns to order items based on pairwise comparisons. In the context of LLMs, for a prompt like “What is the capital of France?” with responses “Paris” (preferred) and “London” (dispreferred), ORPO adjusts the model to increase the probability of “Paris” relative to “London” using the odds ratio, ensuring alignment with human preferences without additional phases.</p> <p>The thinking trace highlights that this approach simplifies the alignment process by leveraging the odds ratio, which provides a statistical measure of relative likelihood, potentially offering a more nuanced adjustment compared to DPO’s log probability difference, as seen in blog posts like "ORPO, DPO, and PPO: Optimizing Models for Human Preferences" (<a href=https://blog.fotiecodes.com/orpo-dpo-and-ppo-optimizing-models-for-human-preferences-cm38nqzki000z09l23tay04ev>ORPO, DPO, and PPO: Optimizing Models for Human Preferences</a>).</p> <p><strong>Comparison with Other Methods</strong></p> <p>To contextualize ORPO, the thinking trace compares it with SFT, RLHF, and DPO, as shown in the following table:</p> <table> <thead> <tr> <th><strong>Method</strong></th> <th><strong>Preference Data</strong></th> <th><strong>Reward Model</strong></th> <th><strong>Optimization Approach</strong></th> <th><strong>Complexity</strong></th> <th><strong>Efficiency</strong></th> </tr> </thead> <tbody> <tr> <td>Supervised Fine-Tuning</td> <td>No</td> <td>No</td> <td>Mimic desired outputs</td> <td>Low</td> <td>High</td> </tr> <tr> <td>RLHF</td> <td>Yes</td> <td>Yes</td> <td>Reinforcement learning (e.g., PPO)</td> <td>High</td> <td>Medium</td> </tr> <tr> <td>Direct Preference Optimization (DPO)</td> <td>Yes</td> <td>No</td> <td>Direct preference optimization</td> <td>Medium</td> <td>High</td> </tr> <tr> <td>Odds Ratio Preference Optimization (ORPO)</td> <td>Yes</td> <td>No</td> <td>Odds ratio in SFT</td> <td>Medium</td> <td>High</td> </tr> </tbody> </table> <p>This table, derived from the thinking trace, highlights ORPO’s position as a high-efficiency method, similar to DPO, but with the unique use of odds ratio, potentially offering advantages in certain scenarios, as noted in the arXiv paper’s empirical results.</p> <p><strong>Empirical Results and Case Studies</strong></p> <p>The thinking trace identifies empirical results from the arXiv paper, showing that fine-tuning models like Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback dataset achieves significant improvements in benchmarks such as AlpacaEval 2.0 (up to 12.20%), IFEval (66.19% on instruction-level loose), and MT-Bench (7.32), surpassing state-of-the-art models with more parameters, as detailed in "ORPO: Monolithic Preference Optimization without Reference Model" (<a href=https://arxiv.org/abs/2403.07691>ORPO: Monolithic Preference Optimization without Reference Model</a>). Case studies include the fine-tuning of Mistral-ORPO-alpha and Mistral-ORPO-beta, with model checkpoints available on Hugging Face (<a href=https://huggingface.co/kaist-ai/mistral-orpo-alpha>Mistral-ORPO-alpha</a>, <a href=https://huggingface.co/kaist-ai/mistral-orpo-beta>Mistral-ORPO-beta</a>), demonstrating practical utility.</p> <p><strong>Advantages and Limitations of ORPO</strong></p> <p>The thinking trace identifies several advantages and limitations, providing a balanced view:</p> <p><strong>Advantages:</strong></p> <ol> <li><strong>Efficiency:</strong> By integrating preference alignment into SFT, ORPO reduces the number of training phases, saving time and computational resources, as highlighted in "Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning" (<a href=https://attentions.ai/blog/demystifying-orpo-a-revolutionary-paradigm-in-language-model-fine-tuning/ >Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning</a>).</li> <li><strong>Simplicity:</strong> It avoids the complexity of reinforcement learning and separate reward model training, making it easier to implement, as noted in the Medium article "ORPO, A New Era for LLMs?" (<a href=https://medium.com/@ignacio.de.gregorio.noblejas/orpo-a-new-era-for-llms-31f99acafec5>ORPO, A New Era for LLMs?</a>).</li> <li><strong>Performance:</strong> Initial studies suggest ORPO can achieve or surpass the performance of state-of-the-art models with fewer parameters, as seen in the arXiv paper’s benchmarks.</li> </ol> <p><strong>Limitations:</strong></p> <ol> <li><strong>Data Requirements:</strong> ORPO requires a significant amount of preference data, including both preferred and dispreferred responses, which can be costly to collect, as mentioned in the thinking trace’s consideration of dataset needs.</li> <li><strong>Hyperparameter Tuning:</strong> The weighting factor and the interpretation of odds ratio probabilities need careful tuning, potentially complicating implementation, as noted in the paper’s discussion.</li> </ol> <p>λ</p> <ol> <li><strong>Theoretical Nuances:</strong> The use of odds ratio, while innovative, may not always provide clear advantages over DPO, especially when sequence probabilities are small, as explored in the thinking trace’s analysis.</li> </ol> <p><strong>Implementation Hints for Practitioners</strong></p> <p>While the user’s request focuses on theory, the thinking trace considers practical implementation for completeness. To implement ORPO, one needs:</p> <ol> <li>A pre-trained language model, such as those available in the Hugging Face Transformers library.</li> <li>A dataset of prompts with preferred and dispreferred responses, which can be collected through human annotation or existing datasets like UltraFeedback.</li> <li>Define the loss function as outlined, using libraries like PyTorch for gradient descent, with the odds ratio calculated as per the paper’s formulation.</li> <li>Tune hyperparameters, including , based on validation performance, as seen in the paper’s experimental setup.</li> </ol> <p>λ</p> <p>The thinking trace also mentions that the code for ORPO is available on GitHub (<a href=https://github.com/xfactlab/orpo>ORPO GitHub</a>), providing a practical resource for practitioners, as noted in the arXiv paper.</p> <p><strong>Conclusion and Future Directions</strong></p> <p>ORPO represents a significant advancement in aligning LLMs with human preferences, offering a more efficient and effective alternative to RLHF and DPO by integrating preference optimization into SFT using the odds ratio. Its mathematical formulation, empirical results, and potential for reducing training costs highlight its importance in the field. Future research may focus on improving data efficiency, addressing the nuances of odds ratio in sequence probabilities, and extending ORPO to multimodal tasks, building on the insights from this analysis.</p> <p>This comprehensive analysis provides a detailed, beginner-friendly introduction to ORPO, covering all aspects requested by the user, including theory, math, intuition, and practical examples, ensuring a thorough understanding for readers interested in AI alignment.</p> <p><strong>Key Citations</strong></p> <ul> <li><a href=https://arxiv.org/abs/2403.07691>ORPO: Monolithic Preference Optimization without Reference Model</a></li> <li><a href=https://attentions.ai/blog/demystifying-orpo-a-revolutionary-paradigm-in-language-model-fine-tuning/ >Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning</a></li> <li><a href=https://medium.com/@ignacio.de.gregorio.noblejas/orpo-a-new-era-for-llms-31f99acafec5>ORPO, A New Era for LLMs?</a></li> <li><a href=https://blog.fotiecodes.com/orpo-dpo-and-ppo-optimizing-models-for-human-preferences-cm38nqzki000z09l23tay04ev>ORPO, DPO, and PPO: Optimizing Models for Human Preferences</a></li> <li><a href=https://huggingface.co/kaist-ai/mistral-orpo-alpha>Mistral-ORPO-alpha</a></li> <li><a href=https://huggingface.co/kaist-ai/mistral-orpo-beta>Mistral-ORPO-beta</a></li> <li><a href=https://github.com/xfactlab/orpo>ORPO GitHub</a></li> </ul></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="March 6, 2025 19:59:31">March 6, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="March 6, 2025 19:59:31">March 6, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../DPO/ class="md-footer__link md-footer__link--prev" aria-label="Previous: DPO(Direct Preference Optimization)"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> DPO(Direct Preference Optimization) </div> </div> </a> <a href=../GRPO/ class="md-footer__link md-footer__link--next" aria-label="Next: Theory Behind GRPO"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Theory Behind GRPO </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Adithya S Kolavi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"/></svg> </a> <a href=https://x.com/adithya_s_k target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../assets/javascripts/custom.9e5da760.min.js></script> <!-- Rich Snippets / Structured Data --> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "EducationalOrganization",
    "name": "AI Engineering Academy",
    "url": "https://aiengineering.academy",
    "logo": "https://aiengineering.academy/assets/logo.png",
    "description": "A structured learning platform for AI engineers with clear paths in prompt engineering, RAG, fine-tuning, deployment, and agent development.",
    "sameAs": [
      "https://github.com/adithya-s-k/AI-Engineering.academy",
      "https://x.com/adithya_s_k"
    ],
    "founder": {
      "@type": "Person",
      "name": "Adithya S Kolavi"
    },
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    }
  }
</script> </body> </html>