<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Mastering Applied AI, One Concept at a Time"><meta name=author content="Adithya S Kolavi"><link href=https://aiengineering.academy/LLM/HandsOnWithFinetuning/SFT/SFT/ rel=canonical><link href=../../../TheoryBehindFinetuning/SFT/ rel=prev><link href=../SFT_finetuning_notebook/ rel=next><link rel=icon href=../../../../assets/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.7"><title>Hands on with SFT - AI Engineering Academy</title><link rel=stylesheet href=../../../../assets/stylesheets/main.8608ea7d.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../../stylesheets/extra.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JP3605WT7D"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JP3605WT7D",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JP3605WT7D",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="Hands on with SFT - AI Engineering Academy"><meta property=og:description content="Mastering Applied AI, One Concept at a Time"><meta property=og:image content=https://aiengineering.academy/assets/images/social/LLM/HandsOnWithFinetuning/SFT/SFT.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://aiengineering.academy/LLM/HandsOnWithFinetuning/SFT/SFT/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Hands on with SFT - AI Engineering Academy"><meta name=twitter:description content="Mastering Applied AI, One Concept at a Time"><meta name=twitter:image content=https://aiengineering.academy/assets/images/social/LLM/HandsOnWithFinetuning/SFT/SFT.png><link rel=stylesheet href=../../../../assets/stylesheets/custom.7c86dd97.min.css><!-- PostHog Analytics --><script>
  !(function (t, e) {
    var o, n, p, r;
    e.__SV ||
      ((window.posthog = e),
      (e._i = []),
      (e.init = function (i, s, a) {
        function g(t, e) {
          var o = e.split(".");
          2 == o.length && ((t = t[o[0]]), (e = o[1])),
            (t[e] = function () {
              t.push([e].concat(Array.prototype.slice.call(arguments, 0)));
            });
        }
        ((p = t.createElement("script")).type = "text/javascript"),
          (p.async = !0),
          (p.src = s.api_host + "/static/array.js"),
          (r = t.getElementsByTagName("script")[0]).parentNode.insertBefore(
            p,
            r
          );
        var u = e;
        for (
          void 0 !== a ? (u = e[a] = []) : (a = "posthog"),
            u.people = u.people || [],
            u.toString = function (t) {
              var e = "posthog";
              return (
                "posthog" !== a && (e += "." + a), t || (e += " (stub)"), e
              );
            },
            u.people.toString = function () {
              return u.toString(1) + ".people (stub)";
            },
            o =
              "capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset isFeatureEnabled onFeatureFlags getFeatureFlag getFeatureFlagPayload reloadFeatureFlags group updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures getActiveMatchingSurveys getSurveys onSessionId".split(
                " "
              ),
            n = 0;
          n < o.length;
          n++
        )
          g(u, o[n]);
        e._i.push([i, s, a]);
      }),
      (e.__SV = 1));
  })(document, window.posthog || []);
  posthog.init("YOUR_POSTHOG_KEY", { api_host: "https://app.posthog.com" });
</script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@adithya_s_k</strong> on <a href=https://x.com/adithya_s_k> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title="AI Engineering Academy" class="md-header__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Engineering Academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Hands on with SFT </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../PromptEngineering/ class=md-tabs__link> Prompt Engineering </a> </li> <li class=md-tabs__item> <a href=../../../../RAG/ class=md-tabs__link> RAG </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../ class=md-tabs__link> LLM </a> </li> <li class=md-tabs__item> <a href=../../../../Deployment/ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../../../Agents/ class=md-tabs__link> Agents </a> </li> <li class=md-tabs__item> <a href=../../../../Projects/ class=md-tabs__link> Projects </a> </li> <li class=md-tabs__item> <a href=../../../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="AI Engineering Academy" class="md-nav__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../../../assets/logo.png alt=logo> </a> AI Engineering Academy </label> <div class=md-nav__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../PromptEngineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../RAG/ class=md-nav__link> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <div class="md-nav__link md-nav__container"> <a href=../../../ class="md-nav__link "> <span class=md-ellipsis> LLM </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2 checked> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex> <span class=md-ellipsis> Finetuning Techniques </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=true> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Finetuning Techniques </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../TheoryBehindFinetuning/PreTrain/ class=md-nav__link> <span class=md-ellipsis> PreTraining LLMs </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_2 checked> <label class=md-nav__link for=__nav_4_2_2 id=__nav_4_2_2_label tabindex=0> <span class=md-ellipsis> SFT </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_2_label aria-expanded=true> <label class=md-nav__title for=__nav_4_2_2> <span class="md-nav__icon md-icon"></span> SFT </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../TheoryBehindFinetuning/SFT/ class=md-nav__link> <span class=md-ellipsis> Theory behind SFT </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Hands on with SFT </span> </a> </li> <li class=md-nav__item> <a href=../SFT_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Finetune Any LLM with SFT </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../TheoryBehindFinetuning/PPO/ class=md-nav__link> <span class=md-ellipsis> PPO(Proximal Policy Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../../../TheoryBehindFinetuning/DPO/ class=md-nav__link> <span class=md-ellipsis> DPO(Direct Preference Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../../../TheoryBehindFinetuning/ORPO/ class=md-nav__link> <span class=md-ellipsis> ORPO(Odds Ratio Preference Optimization) </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../TheoryBehindFinetuning/GRPO/ class=md-nav__link> <span class=md-ellipsis> GRPO(Group Relative Policy Optimization) </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> LLM Finetuning Hands on </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> LLM Finetuning Hands on </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Gemma/ class=md-nav__link> <span class=md-ellipsis> Gemma </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../LLama2/Llama2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Llama2 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Llama3_finetuning_notebook.ipynb class=md-nav__link> <span class=md-ellipsis> Llama3 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Mistral-7b/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> VLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> VLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../VLM/Florence2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Florence2 </span> </a> </li> <li class=md-nav__item> <a href=../../../VLM/PaliGemma_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> PaliGemma </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_5> <div class="md-nav__link md-nav__container"> <a href=../../../LLMArchitecture/ParameterCount/ class="md-nav__link "> <span class=md-ellipsis> LLM Architecture </span> </a> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> LLM Architecture </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../Deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../Agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../Projects/ class=md-nav__link> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/edit/master/docs/LLM/HandsOnWithFinetuning/SFT/SFT.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/raw/master/docs/LLM/HandsOnWithFinetuning/SFT/SFT.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1>Hands on with SFT</h1> <div><p><strong>Comprehensive Hands On with Supervised Fine-Tuning for LLMs</strong></p> <p>This section provides a detailed exploration of Supervised Fine-Tuning (SFT) for Large Language Models (LLMs), covering its definition, process, applications, challenges, and best practices. It aims to offer a thorough understanding for researchers, developers, and practitioners, building on the key points and expanding with technical details and examples, as of 10:53 PM IST on Monday, February 24, 2025.</p> <p><strong>Definition and Context</strong></p> <p>Large Language Models (LLMs) are neural networks trained on vast text corpora using self-supervision, such as predicting the next word in a sequence. Examples include models like GPT-3, BERT, and RoBERTa, which are initially trained without explicit labels. Supervised Fine-Tuning, however, involves taking these pre-trained models and further training them on a labeled dataset for a specific task or domain. This process, often referred to as SFT, uses labeled data—pairs of inputs and their corresponding outputs—to adapt the model’s weights, enabling it to learn task-specific patterns and nuances.</p> <p>This differs from unsupervised fine-tuning, which uses unlabeled data (e.g., masked language modeling), and reinforcement learning-based fine-tuning, such as Reinforcement Learning from Human Feedback (RLHF), which optimizes based on a reward signal. SFT is particularly effective when labeled data is available, making it a straightforward and powerful method for task-specific adaptation.</p> <p><strong>Importance and Motivation</strong></p> <p>Pre-trained LLMs are generalists, capable of handling a wide range of language tasks but often underperforming on specific applications without further tuning. SFT addresses this by tailoring the model to excel in areas like text classification, named entity recognition, question-answering, summarization, translation, and chatbot development. For instance, a model fine-tuned for medical terminology can interpret and generate domain-specific jargon better than a generic model, enhancing its utility in healthcare applications.</p> <p>The importance lies in its efficiency and adaptability. As noted in resources like <a href=https://www.superannotate.com/blog/llm-fine-tuning>SuperAnnotate: Fine-tuning large language models (LLMs) in 2024</a>, SFT can significantly improve performance with relatively small datasets, often requiring 50-100,000 examples for multi-task learning or just a few hundred to thousands for task-specific fine-tuning. This efficiency is crucial for businesses with limited data and computational resources, making LLMs accessible for specialized applications.</p> <p><strong>Process and Techniques</strong></p> <p>The process of SFT can be broken down into several stages, each critical for success:</p> <ol> <li><strong>Data Collection and Preparation</strong>:</li> <li>Gather a labeled dataset relevant to the task, such as prompt-response pairs for instruction fine-tuning or input-output pairs for classification. Open-source datasets like <a href=https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations>GPT-4all Dataset</a>, <a href=https://github.com/gururise/AlpacaDataCleaned>AlpacaDataCleaned</a>, and <a href=https://huggingface.co/datasets/databricks/databricks-dolly-15k>databricks-dolly-15k</a> are commonly used.</li> <li>Preprocess the data by cleaning, tokenizing, and formatting it to ensure compatibility with the model. This step is vital, as data quality directly impacts performance, with challenges like inconsistencies, bias, and missing values needing attention (as highlighted in <a href=https://kili-technology.com/large-language-models-llms/the-ultimate-guide-to-fine-tuning-llms-2024>Kili Technology: What is LLM Fine-Tuning?</a>).</li> <li><strong>Model Selection</strong>:</li> <li>Choose a pre-trained LLM based on task requirements, model size, and computational resources. Larger models like GPT-3 offer higher accuracy but require more resources, while smaller models may suffice for specific tasks. Factors like data type, desired outcomes, and budget are crucial, as discussed in <a href=https://www.sama.com/blog/supervised-fine-tuning-how-to-choose-the-right-llm>Sama: Supervised Fine-Tuning: How to choose the right LLM</a>.</li> <li><strong>Fine-Tuning</strong>:</li> <li>Train the model on the task-specific dataset using supervised learning techniques. The model’s weights are adjusted based on the gradients derived from a task-specific loss function, measuring the difference between predictions and ground truth labels. Optimization algorithms like gradient descent are used over multiple epochs to adapt the model.</li> <li>Several techniques enhance this process:<ul> <li><strong>Instruction Fine-Tuning</strong>: Trains the model with examples that include instructions, such as “summarize this text,” to improve task-specific responses (e.g., <a href=https://www.superannotate.com/blog/llm-fine-tuning>SuperAnnotate: Fine-tuning large language models (LLMs) in 2024</a>).</li> <li><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong>: Methods like LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) reduce the number of trainable parameters, making fine-tuning more efficient. LoRA adds adapters with weights, freezing the rest, while QLoRA quantizes weights to 4-bit precision, reducing memory usage (as detailed in <a href=https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3>Medium: Supervised Fine-tuning: customizing LLMs</a>).</li> <li><strong>Batch Packing</strong>: Combines inputs to increase batch capabilities, optimizing computational resources (mentioned in the same Medium article).</li> <li><strong>Half Fine-Tuning (HFT)</strong>: Freezes half the parameters per round while updating the other half, balancing knowledge retention and new skill acquisition, as noted in <a href=https://arxiv.org/pdf/2408.13296.pdf>arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs</a>.</li> </ul> </li> <li><strong>Evaluation</strong>:</li> <li>Test the fine-tuned model on a validation set to assess performance metrics like accuracy, F1 score, or BLEU for translation tasks. This step ensures the model generalizes well to unseen data.</li> <li><strong>Deployment</strong>:</li> <li>Once validated, deploy the model for real-world use, integrating it into applications like chatbots, content generators, or customer support systems.</li> </ol> <p><strong>Applications and Examples</strong></p> <p>SFT is versatile, applicable to a wide range of tasks:</p> <ul> <li><strong>Text Classification</strong>: Classifying documents into categories, such as sentiment analysis on movie reviews.</li> <li><strong>Named Entity Recognition</strong>: Identifying entities like names, dates, and locations in text.</li> <li><strong>Question-Answering</strong>: Providing accurate answers to specific queries, enhancing virtual assistants.</li> <li><strong>Summarization</strong>: Generating concise summaries of longer texts, useful for news aggregation.</li> <li><strong>Translation</strong>: Translating text between languages, improving multilingual communication.</li> <li><strong>Chatbots</strong>: Creating conversational agents for specific domains, like customer support or healthcare.</li> </ul> <p>A practical example is fine-tuning a pre-trained model for a science educational platform. Initially, it might answer “Why is the sky blue?” with a simple “Because of the way the atmosphere scatters sunlight.” After SFT with labeled data, it provides a detailed response: “The sky appears blue because of Rayleigh scattering... blue light has a shorter wavelength and is scattered... causing the sky to take on a blue hue” (<a href=https://www.superannotate.com/blog/llm-fine-tuning>SuperAnnotate: Fine-tuning large language models (LLMs) in 2024</a>).</p> <p>Another example is fine-tuning RoBERTa for sentiment analysis, where the model learns from labeled movie reviews to classify sentiments as positive, negative, or neutral, significantly improving accuracy compared to the base model.</p> <p><strong>Challenges and Best Practices</strong></p> <p>Despite its benefits, SFT faces several challenges:</p> <ul> <li><strong>Catastrophic Forgetting</strong>: The model may forget general knowledge while focusing on task-specific learning, a concern addressed by methods like Half Fine-Tuning (HFT) (<a href=https://arxiv.org/pdf/2408.13296.pdf>arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs</a>).</li> <li><strong>Data Quality</strong>: Ensuring high-quality labeled data is crucial, with challenges like inconsistencies, bias, and data scarcity in specific domains. Automated tools like the Kili app can help streamline data curation (<a href=https://kili-technology.com/large-language-models-llms/the-ultimate-guide-to-fine-tuning-llms-2024>Kili Technology: What is LLM Fine-Tuning?</a>).</li> <li><strong>Computational Resources</strong>: Fine-tuning large models is resource-intensive, necessitating efficient methods like PEFT to reduce costs.</li> </ul> <p>Best practices include:</p> <ul> <li><strong>Choosing the Right Model</strong>: Consider model size, performance on similar tasks, and computational resources. Larger models offer higher accuracy but require more resources (<a href=https://www.sama.com/blog/supervised-fine-tuning-how-to-choose-the-right-llm>Sama: Supervised Fine-Tuning: How to choose the right LLM</a>).</li> <li><strong>Data Preparation</strong>: Ensure the dataset is clean, representative, and sufficient, with splits for training, validation, and testing. For instance, 50-100,000 examples may be needed for multi-task learning (<a href=https://www.superannotate.com/blog/llm-fine-tuning>SuperAnnotate: Fine-tuning large language models (LLMs) in 2024</a>).</li> <li><strong>Hyperparameter Tuning</strong>: Optimize learning rate, batch size, and number of epochs to avoid overfitting or underfitting, as discussed in <a href=https://datasciencedojo.com/blog/fine-tuning-llms/ >Data Science Dojo: Fine-tuning LLMs 101</a>.</li> <li><strong>Use Parameter-Efficient Methods</strong>: Techniques like LoRA and QLoRA can reduce trainable parameters by up to 10,000 times, making fine-tuning feasible on limited hardware (<a href=https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3>Medium: Supervised Fine-tuning: customizing LLMs</a>).</li> </ul> <p><strong>Comparative Analysis and Recent Advancements</strong></p> <p>SFT contrasts with other methods like unsupervised fine-tuning (using unlabeled data) and RLHF (optimizing based on rewards). Its advantage lies in its direct use of labeled data, making it suitable for tasks with clear input-output mappings. Recent advancements, such as the TRL library and AutoTrain Advanced tool by Hugging Face, facilitate the process, offering resources for developers (<a href=https://huggingface.co/blog/rishiraj/finetune-llms>Hugging Face: Fine-Tuning LLMs: Supervised Fine-Tuning and Reward Modelling</a>).</p> <p>Techniques like instruction fine-tuning and PEFT are gaining traction, with research showing significant performance improvements. For example, QLoRA can fine-tune a high-quality chatbot in 24 hours on a single GPU, demonstrating efficiency (<a href=https://arxiv.org/pdf/2408.13296.pdf>arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs</a>).</p> <p><strong>Hands-On Code for SFT Using Hugging Face Libraries</strong></p> <p>Given the focus on hands-on code using Hugging Face’s transformers, PEFT, bitsandbytes, and TRL libraries, here’s a detailed example for fine-tuning an LLM, specifically using gpt2 as the base model, with comments for alternative models. This example demonstrates how to format data in prompt format and perform SFT.</p> <p><strong>Environment Setup</strong></p> <p>First, install the necessary libraries:</p> <p>bash</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>pip<span class=w> </span>install<span class=w> </span>transformers<span class=w> </span>peft<span class=w> </span>bitsandbytes<span class=w> </span>trl
</span></code></pre></div> <p><strong>Data Preparation</strong></p> <p>The data for SFT should be in a format where each sample consists of an input (prompt) and the corresponding output (completion). For instruction-following tasks, the prompt might be “Instruction: Summarize this text. Text: [some text]” and the output is the summary. Popular datasets include <a href=https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations>GPT-4all Dataset</a>, <a href=https://github.com/gururise/AlpacaDataCleaned>AlpacaDataCleaned</a>, and <a href=https://huggingface.co/datasets/databricks/databricks-dolly-15k>databricks-dolly-15k</a>.</p> <p>For this example, let’s create a small dataset:</p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>Dataset</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>data</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>    <span class=p>{</span><span class=s2>"instruction"</span><span class=p>:</span> <span class=s2>"What is the capital of France?"</span><span class=p>,</span> <span class=s2>"output"</span><span class=p>:</span> <span class=s2>"Paris"</span><span class=p>},</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>    <span class=p>{</span><span class=s2>"instruction"</span><span class=p>:</span> <span class=s2>"What is the largest planet in our solar system?"</span><span class=p>,</span> <span class=s2>"output"</span><span class=p>:</span> <span class=s2>"Jupiter"</span><span class=p>},</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=p>]</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>Dataset</span><span class=o>.</span><span class=n>from_list</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></code></pre></div> <p>The dataset should have columns for the prompt (e.g., “instruction”) and completion (e.g., “output”), which will be specified in the trainer.</p> <p><strong>Model Selection and Fine-Tuning</strong></p> <p>Choose a pre-trained model from Hugging Face’s model hub. For this example, we’ll use gpt2, but other models like gpt2-large, bloom-560m, or distilbert-base-uncased can be used (commented out for reference):</p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=c1># Choose the model (uncomment other options as needed)</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=n>model_name</span> <span class=o>=</span> <span class=s2>"gpt2"</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=c1># model_name = "gpt2-large"  # Larger model for better performance</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=c1># model_name = "bloom-560m"  # Another option for decoder-only models</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=c1># model_name = "distilbert-base-uncased"  # For encoder-based tasks, though less common for SFT</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=c1># For larger models, use bitsandbytes for quantization to save memory</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>BitsAndBytesConfig</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a>
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a><span class=n>quantization_config</span> <span class=o>=</span> <span class=n>BitsAndBytesConfig</span><span class=p>(</span>
</span><span id=__span-2-16><a id=__codelineno-2-16 name=__codelineno-2-16 href=#__codelineno-2-16></a>    <span class=n>load_in_8bit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-2-17><a id=__codelineno-2-17 name=__codelineno-2-17 href=#__codelineno-2-17></a>    <span class=n>llm_int8_threshold</span><span class=o>=</span><span class=mf>6.0</span><span class=p>,</span>
</span><span id=__span-2-18><a id=__codelineno-2-18 name=__codelineno-2-18 href=#__codelineno-2-18></a><span class=p>)</span>
</span><span id=__span-2-19><a id=__codelineno-2-19 name=__codelineno-2-19 href=#__codelineno-2-19></a><span class=c1># model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)  # Uncomment for 8-bit loading</span>
</span><span id=__span-2-20><a id=__codelineno-2-20 name=__codelineno-2-20 href=#__codelineno-2-20></a>
</span><span id=__span-2-21><a id=__codelineno-2-21 name=__codelineno-2-21 href=#__codelineno-2-21></a><span class=c1># Set up PEFT for parameter-efficient fine-tuning using LoRA</span>
</span><span id=__span-2-22><a id=__codelineno-2-22 name=__codelineno-2-22 href=#__codelineno-2-22></a><span class=kn>from</span><span class=w> </span><span class=nn>peft</span><span class=w> </span><span class=kn>import</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>get_peft_model</span>
</span><span id=__span-2-23><a id=__codelineno-2-23 name=__codelineno-2-23 href=#__codelineno-2-23></a>
</span><span id=__span-2-24><a id=__codelineno-2-24 name=__codelineno-2-24 href=#__codelineno-2-24></a><span class=n>lora_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
</span><span id=__span-2-25><a id=__codelineno-2-25 name=__codelineno-2-25 href=#__codelineno-2-25></a>    <span class=n>r</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span><span id=__span-2-26><a id=__codelineno-2-26 name=__codelineno-2-26 href=#__codelineno-2-26></a>    <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span><span id=__span-2-27><a id=__codelineno-2-27 name=__codelineno-2-27 href=#__codelineno-2-27></a>    <span class=n>target_modules</span><span class=o>=</span><span class=p>[</span><span class=s2>"q_proj"</span><span class=p>,</span> <span class=s2>"v_proj"</span><span class=p>],</span>
</span><span id=__span-2-28><a id=__codelineno-2-28 name=__codelineno-2-28 href=#__codelineno-2-28></a>    <span class=n>lora_dropout</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span>
</span><span id=__span-2-29><a id=__codelineno-2-29 name=__codelineno-2-29 href=#__codelineno-2-29></a>    <span class=n>bias</span><span class=o>=</span><span class=s2>"none"</span><span class=p>,</span>
</span><span id=__span-2-30><a id=__codelineno-2-30 name=__codelineno-2-30 href=#__codelineno-2-30></a><span class=p>)</span>
</span><span id=__span-2-31><a id=__codelineno-2-31 name=__codelineno-2-31 href=#__codelineno-2-31></a><span class=n>model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>lora_config</span><span class=p>)</span>
</span></code></pre></div> <p><strong>Training with TRL’s SFTTrainer</strong></p> <p>Use TRL’s SFTTrainer for supervised fine-tuning, specifying the prompt and completion columns:</p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=kn>from</span><span class=w> </span><span class=nn>trl</span><span class=w> </span><span class=kn>import</span> <span class=n>SFTTrainer</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>TrainingArguments</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>"path/to/output/dir"</span><span class=p>,</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>    <span class=n>overwrite_output_dir</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>    <span class=n>num_train_steps</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>,</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>    <span class=n>logging_steps</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a>    <span class=n>save_steps</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a>    <span class=n>save_total_limit</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>    <span class=n>fp16</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a><span class=p>)</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a>
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17 href=#__codelineno-3-17></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>SFTTrainer</span><span class=p>(</span>
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18 href=#__codelineno-3-18></a>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span><span id=__span-3-19><a id=__codelineno-3-19 name=__codelineno-3-19 href=#__codelineno-3-19></a>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span>
</span><span id=__span-3-20><a id=__codelineno-3-20 name=__codelineno-3-20 href=#__codelineno-3-20></a>    <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span><span id=__span-3-21><a id=__codelineno-3-21 name=__codelineno-3-21 href=#__codelineno-3-21></a>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-3-22><a id=__codelineno-3-22 name=__codelineno-3-22 href=#__codelineno-3-22></a>    <span class=n>prompt_column</span><span class=o>=</span><span class=s2>"instruction"</span><span class=p>,</span>
</span><span id=__span-3-23><a id=__codelineno-3-23 name=__codelineno-3-23 href=#__codelineno-3-23></a>    <span class=n>completion_column</span><span class=o>=</span><span class=s2>"output"</span><span class=p>,</span>
</span><span id=__span-3-24><a id=__codelineno-3-24 name=__codelineno-3-24 href=#__codelineno-3-24></a><span class=p>)</span>
</span><span id=__span-3-25><a id=__codelineno-3-25 name=__codelineno-3-25 href=#__codelineno-3-25></a>
</span><span id=__span-3-26><a id=__codelineno-3-26 name=__codelineno-3-26 href=#__codelineno-3-26></a><span class=c1># Train the model</span>
</span><span id=__span-3-27><a id=__codelineno-3-27 name=__codelineno-3-27 href=#__codelineno-3-27></a><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-3-28><a id=__codelineno-3-28 name=__codelineno-3-28 href=#__codelineno-3-28></a>
</span><span id=__span-3-29><a id=__codelineno-3-29 name=__codelineno-3-29 href=#__codelineno-3-29></a><span class=c1># Save the fine-tuned model</span>
</span><span id=__span-3-30><a id=__codelineno-3-30 name=__codelineno-3-30 href=#__codelineno-3-30></a><span class=n>trainer</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>"path/to/save/model"</span><span class=p>)</span>
</span></code></pre></div> <p>This code demonstrates how to use transformers, PEFT, and TRL for SFT, with bitsandbytes for optional quantization. The SFTTrainer handles the formatting of prompts and completions, making the process streamlined.</p> <p><strong>Evaluation and Deployment</strong></p> <p>After training, evaluate the model on a validation set using metrics like accuracy, F1 score, or BLEU, depending on the task. For deployment, integrate the model into applications like chatbots or content generators, ensuring it performs well in real-world scenarios.</p> <p><strong>Summary Table of Key Techniques</strong></p> <table> <thead> <tr> <th><strong>Technique</strong></th> <th><strong>Description</strong></th> <th><strong>Use Case</strong></th> </tr> </thead> <tbody> <tr> <td>LoRA (Low-Rank Adaptation)</td> <td>Updates fewer parameters, making fine-tuning efficient</td> <td>Resource-constrained environments</td> </tr> <tr> <td>QLoRA (Quantized LoRA)</td> <td>Combines LoRA with 4-bit quantization for memory efficiency</td> <td>Large model fine-tuning</td> </tr> <tr> <td>Instruction Fine-Tuning</td> <td>Trains with examples including instructions for task-specific responses</td> <td>Chatbots, question-answering</td> </tr> <tr> <td>Batch Packing</td> <td>Combines inputs to optimize computational resources</td> <td>High-throughput training</td> </tr> <tr> <td>Half Fine-Tuning (HFT)</td> <td>Freezes half parameters per round to balance knowledge retention and learning</td> <td>Preventing catastrophic forgetting</td> </tr> </tbody> </table> <p>This table summarizes the key techniques discussed, aiding in understanding their applications.</p> <p><strong>Formatting the Dataset for Instruction Fine-Tuning</strong></p> <p>Proper dataset formatting is crucial for SFT. Using the <a href=https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style>TokenBender/code_instructions_122k_alpaca_style</a> dataset, we’ll convert instruction-output pairs into a "prompt" format tailored for instruction fine-tuning. Here’s how:</p> <p><strong>Step 1: Load the Dataset</strong></p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=c1># Load the dataset</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>"TokenBender/code_instructions_122k_alpaca_style"</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>"train"</span><span class=p>)</span>
</span></code></pre></div> <p>This dataset contains 122,000 entries with columns: instruction, input, output, and text. For example:</p> <ul> <li><strong>Instruction</strong>: "Create a function to calculate the sum of a sequence of integers"</li> <li><strong>Input</strong>: "[1, 2, 3, 4, 5]"</li> <li><strong>Output</strong>: "# Python code\ndef sum_sequence(sequence):\n sum = 0\n for num in sequence:\n sum += num\n return sum"</li> </ul> <p><strong>Step 2: Define the Prompt Generation Function</strong></p> <p>We’ll create a generate_prompt function to format each entry into a structured prompt, distinguishing user instructions from model responses:</p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=k>def</span><span class=w> </span><span class=nf>generate_prompt</span><span class=p>(</span><span class=n>data_point</span><span class=p>):</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=w>    </span><span class=sd>"""Generate a prompt from instruction, input, and output."""</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>    <span class=n>prefix_text</span> <span class=o>=</span> <span class=s1>'Below is an instruction that describes a task. Write a response that appropriately completes the request.</span><span class=se>\n\n</span><span class=s1>'</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>    <span class=k>if</span> <span class=n>data_point</span><span class=p>[</span><span class=s1>'input'</span><span class=p>]</span> <span class=ow>and</span> <span class=n>data_point</span><span class=p>[</span><span class=s1>'input'</span><span class=p>]</span> <span class=o>!=</span> <span class=s2>"Not applicable"</span><span class=p>:</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>        <span class=n>text</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"&lt;start_of_turn&gt;user </span><span class=si>{</span><span class=n>prefix_text</span><span class=si>}{</span><span class=n>data_point</span><span class=p>[</span><span class=s1>'instruction'</span><span class=p>]</span><span class=si>}</span><span class=s2> here are the inputs </span><span class=si>{</span><span class=n>data_point</span><span class=p>[</span><span class=s1>'input'</span><span class=p>]</span><span class=si>}</span><span class=s2> &lt;end_of_turn&gt;</span><span class=se>\n</span><span class=s2>&lt;start_of_turn&gt;model</span><span class=si>{</span><span class=n>data_point</span><span class=p>[</span><span class=s1>'output'</span><span class=p>]</span><span class=si>}</span><span class=s2> &lt;end_of_turn&gt;"</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>        <span class=n>text</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"&lt;start_of_turn&gt;user </span><span class=si>{</span><span class=n>prefix_text</span><span class=si>}{</span><span class=n>data_point</span><span class=p>[</span><span class=s1>'instruction'</span><span class=p>]</span><span class=si>}</span><span class=s2> &lt;end_of_turn&gt;</span><span class=se>\n</span><span class=s2>&lt;start_of_turn&gt;model</span><span class=si>{</span><span class=n>data_point</span><span class=p>[</span><span class=s1>'output'</span><span class=p>]</span><span class=si>}</span><span class=s2> &lt;end_of_turn&gt;"</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>    <span class=k>return</span> <span class=n>text</span>
</span></code></pre></div> <ul> <li><strong>With Input</strong>: Includes the input in the prompt (e.g., "[1, 2, 3, 4, 5]").</li> <li><strong>Without Input</strong>: Omits the input section if it’s empty or "Not applicable".</li> </ul> <p><strong>Step 3: Add Prompt Column, Shuffle, Tokenize, and Split</strong></p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># Add "prompt" column</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=n>text_column</span> <span class=o>=</span> <span class=p>[</span><span class=n>generate_prompt</span><span class=p>(</span><span class=n>data_point</span><span class=p>)</span> <span class=k>for</span> <span class=n>data_point</span> <span class=ow>in</span> <span class=n>dataset</span><span class=p>]</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>add_column</span><span class=p>(</span><span class=s2>"prompt"</span><span class=p>,</span> <span class=n>text_column</span><span class=p>)</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a><span class=c1># Load tokenizer (example with GPT-2)</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>"gpt2"</span><span class=p>)</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>  <span class=c1># Set padding token</span>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a><span class=c1># Shuffle and tokenize</span>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>1234</span><span class=p>)</span>
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>samples</span><span class=p>:</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>samples</span><span class=p>[</span><span class=s2>"prompt"</span><span class=p>],</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s2>"max_length"</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>512</span><span class=p>),</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-6-13><a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a>
</span><span id=__span-6-14><a id=__codelineno-6-14 name=__codelineno-6-14 href=#__codelineno-6-14></a><span class=c1># Split into train (80%) and test (20%)</span>
</span><span id=__span-6-15><a id=__codelineno-6-15 name=__codelineno-6-15 href=#__codelineno-6-15></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>train_test_split</span><span class=p>(</span><span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>)</span>
</span><span id=__span-6-16><a id=__codelineno-6-16 name=__codelineno-6-16 href=#__codelineno-6-16></a><span class=n>train_data</span> <span class=o>=</span> <span class=n>dataset</span><span class=p>[</span><span class=s2>"train"</span><span class=p>]</span>
</span><span id=__span-6-17><a id=__codelineno-6-17 name=__codelineno-6-17 href=#__codelineno-6-17></a><span class=n>test_data</span> <span class=o>=</span> <span class=n>dataset</span><span class=p>[</span><span class=s2>"test"</span><span class=p>]</span>
</span></code></pre></div> <p><strong>Resulting Prompt Example</strong></p> <p>For the first entry:</p> <p>plaintext</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>&lt;start_of_turn&gt;user Below is an instruction that describes a task. Write a response that appropriately completes the request.
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>Create a function to calculate the sum of a sequence of integers here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt;
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>&lt;start_of_turn&gt;model # Python code
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>def sum_sequence(sequence):
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a> sum = 0
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a> for num in sequence:
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a> sum += num
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a> return sum &lt;end_of_turn&gt;
</span></code></pre></div> <p>This format helps the model learn to associate instructions (and inputs) with correct outputs.</p> <hr> <p><strong>Example 1: Fine-Tuning for Code Generation</strong></p> <p><strong>Objective</strong></p> <p>Fine-tune a model to generate Python code from natural language instructions using the dataset above.</p> <p><strong>Environment Setup</strong></p> <p>Install required libraries:</p> <p>bash</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>pip<span class=w> </span>install<span class=w> </span>transformers<span class=w> </span>peft<span class=w> </span>bitsandbytes<span class=w> </span>trl<span class=w> </span>datasets
</span></code></pre></div> <p><strong>Code Implementation</strong></p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>TrainingArguments</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class=kn>from</span><span class=w> </span><span class=nn>trl</span><span class=w> </span><span class=kn>import</span> <span class=n>SFTTrainer</span>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a><span class=kn>from</span><span class=w> </span><span class=nn>peft</span><span class=w> </span><span class=kn>import</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>get_peft_model</span>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a><span class=c1># Load model and tokenizer</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a><span class=n>model_name</span> <span class=o>=</span> <span class=s2>"gpt2"</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span><span id=__span-9-8><a id=__codelineno-9-8 name=__codelineno-9-8 href=#__codelineno-9-8></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span><span id=__span-9-9><a id=__codelineno-9-9 name=__codelineno-9-9 href=#__codelineno-9-9></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>
</span><span id=__span-9-10><a id=__codelineno-9-10 name=__codelineno-9-10 href=#__codelineno-9-10></a>
</span><span id=__span-9-11><a id=__codelineno-9-11 name=__codelineno-9-11 href=#__codelineno-9-11></a><span class=c1># Apply LoRA for efficiency</span>
</span><span id=__span-9-12><a id=__codelineno-9-12 name=__codelineno-9-12 href=#__codelineno-9-12></a><span class=n>lora_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
</span><span id=__span-9-13><a id=__codelineno-9-13 name=__codelineno-9-13 href=#__codelineno-9-13></a>    <span class=n>r</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span><span id=__span-9-14><a id=__codelineno-9-14 name=__codelineno-9-14 href=#__codelineno-9-14></a>    <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span><span id=__span-9-15><a id=__codelineno-9-15 name=__codelineno-9-15 href=#__codelineno-9-15></a>    <span class=n>target_modules</span><span class=o>=</span><span class=p>[</span><span class=s2>"c_attn"</span><span class=p>],</span>  <span class=c1># GPT-2 specific</span>
</span><span id=__span-9-16><a id=__codelineno-9-16 name=__codelineno-9-16 href=#__codelineno-9-16></a>    <span class=n>lora_dropout</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span>
</span><span id=__span-9-17><a id=__codelineno-9-17 name=__codelineno-9-17 href=#__codelineno-9-17></a>    <span class=n>bias</span><span class=o>=</span><span class=s2>"none"</span>
</span><span id=__span-9-18><a id=__codelineno-9-18 name=__codelineno-9-18 href=#__codelineno-9-18></a><span class=p>)</span>
</span><span id=__span-9-19><a id=__codelineno-9-19 name=__codelineno-9-19 href=#__codelineno-9-19></a><span class=n>model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>lora_config</span><span class=p>)</span>
</span><span id=__span-9-20><a id=__codelineno-9-20 name=__codelineno-9-20 href=#__codelineno-9-20></a>
</span><span id=__span-9-21><a id=__codelineno-9-21 name=__codelineno-9-21 href=#__codelineno-9-21></a><span class=c1># Training arguments</span>
</span><span id=__span-9-22><a id=__codelineno-9-22 name=__codelineno-9-22 href=#__codelineno-9-22></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-9-23><a id=__codelineno-9-23 name=__codelineno-9-23 href=#__codelineno-9-23></a>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>"./code_gen_model"</span><span class=p>,</span>
</span><span id=__span-9-24><a id=__codelineno-9-24 name=__codelineno-9-24 href=#__codelineno-9-24></a>    <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-9-25><a id=__codelineno-9-25 name=__codelineno-9-25 href=#__codelineno-9-25></a>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-9-26><a id=__codelineno-9-26 name=__codelineno-9-26 href=#__codelineno-9-26></a>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-9-27><a id=__codelineno-9-27 name=__codelineno-9-27 href=#__codelineno-9-27></a>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-4</span><span class=p>,</span>
</span><span id=__span-9-28><a id=__codelineno-9-28 name=__codelineno-9-28 href=#__codelineno-9-28></a>    <span class=n>fp16</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-9-29><a id=__codelineno-9-29 name=__codelineno-9-29 href=#__codelineno-9-29></a>    <span class=n>logging_steps</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span><span id=__span-9-30><a id=__codelineno-9-30 name=__codelineno-9-30 href=#__codelineno-9-30></a>    <span class=n>save_steps</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span>
</span><span id=__span-9-31><a id=__codelineno-9-31 name=__codelineno-9-31 href=#__codelineno-9-31></a><span class=p>)</span>
</span><span id=__span-9-32><a id=__codelineno-9-32 name=__codelineno-9-32 href=#__codelineno-9-32></a>
</span><span id=__span-9-33><a id=__codelineno-9-33 name=__codelineno-9-33 href=#__codelineno-9-33></a><span class=c1># Initialize SFTTrainer</span>
</span><span id=__span-9-34><a id=__codelineno-9-34 name=__codelineno-9-34 href=#__codelineno-9-34></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>SFTTrainer</span><span class=p>(</span>
</span><span id=__span-9-35><a id=__codelineno-9-35 name=__codelineno-9-35 href=#__codelineno-9-35></a>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span><span id=__span-9-36><a id=__codelineno-9-36 name=__codelineno-9-36 href=#__codelineno-9-36></a>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>train_data</span><span class=p>,</span>
</span><span id=__span-9-37><a id=__codelineno-9-37 name=__codelineno-9-37 href=#__codelineno-9-37></a>    <span class=n>eval_dataset</span><span class=o>=</span><span class=n>test_data</span><span class=p>,</span>
</span><span id=__span-9-38><a id=__codelineno-9-38 name=__codelineno-9-38 href=#__codelineno-9-38></a>    <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span><span id=__span-9-39><a id=__codelineno-9-39 name=__codelineno-9-39 href=#__codelineno-9-39></a>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-9-40><a id=__codelineno-9-40 name=__codelineno-9-40 href=#__codelineno-9-40></a>    <span class=n>dataset_text_field</span><span class=o>=</span><span class=s2>"prompt"</span><span class=p>,</span>
</span><span id=__span-9-41><a id=__codelineno-9-41 name=__codelineno-9-41 href=#__codelineno-9-41></a>    <span class=n>max_seq_length</span><span class=o>=</span><span class=mi>512</span>
</span><span id=__span-9-42><a id=__codelineno-9-42 name=__codelineno-9-42 href=#__codelineno-9-42></a><span class=p>)</span>
</span><span id=__span-9-43><a id=__codelineno-9-43 name=__codelineno-9-43 href=#__codelineno-9-43></a>
</span><span id=__span-9-44><a id=__codelineno-9-44 name=__codelineno-9-44 href=#__codelineno-9-44></a><span class=c1># Train the model</span>
</span><span id=__span-9-45><a id=__codelineno-9-45 name=__codelineno-9-45 href=#__codelineno-9-45></a><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-9-46><a id=__codelineno-9-46 name=__codelineno-9-46 href=#__codelineno-9-46></a>
</span><span id=__span-9-47><a id=__codelineno-9-47 name=__codelineno-9-47 href=#__codelineno-9-47></a><span class=c1># Save the model</span>
</span><span id=__span-9-48><a id=__codelineno-9-48 name=__codelineno-9-48 href=#__codelineno-9-48></a><span class=n>trainer</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>"./code_gen_model_final"</span><span class=p>)</span>
</span></code></pre></div> <p><strong>Application Deployment</strong></p> <p><strong>Usage</strong>: Send a POST request with {"instruction": "Create a function to reverse a list", "input": "[1, 2, 3]"} to get Python code as output.</p> <hr> <p><strong>Example 2: Fine-Tuning for Text Summarization</strong></p> <p><strong>Objective</strong></p> <p>Fine-tune a model to summarize text using a synthetic dataset (for illustration).</p> <p><strong>Dataset Preparation</strong></p> <p>Create a small dataset:</p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=n>data</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>    <span class=p>{</span><span class=s2>"instruction"</span><span class=p>:</span> <span class=s2>"Summarize the following text"</span><span class=p>,</span> <span class=s2>"input"</span><span class=p>:</span> <span class=s2>"The quick brown fox jumps over the lazy dog repeatedly."</span><span class=p>,</span> <span class=s2>"output"</span><span class=p>:</span> <span class=s2>"The fox repeatedly jumps over the dog."</span><span class=p>},</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>    <span class=p>{</span><span class=s2>"instruction"</span><span class=p>:</span> <span class=s2>"Summarize the following text"</span><span class=p>,</span> <span class=s2>"input"</span><span class=p>:</span> <span class=s2>"A long story about a hero saving the world."</span><span class=p>,</span> <span class=s2>"output"</span><span class=p>:</span> <span class=s2>"A hero saves the world."</span><span class=p>}</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a><span class=p>]</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>Dataset</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>Dataset</span><span class=o>.</span><span class=n>from_list</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a><span class=n>text_column</span> <span class=o>=</span> <span class=p>[</span><span class=n>generate_prompt</span><span class=p>(</span><span class=n>dp</span><span class=p>)</span> <span class=k>for</span> <span class=n>dp</span> <span class=ow>in</span> <span class=n>dataset</span><span class=p>]</span>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>add_column</span><span class=p>(</span><span class=s2>"prompt"</span><span class=p>,</span> <span class=n>text_column</span><span class=p>)</span>
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>1234</span><span class=p>)</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=s2>"prompt"</span><span class=p>],</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s2>"max_length"</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>128</span><span class=p>),</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-10-10><a id=__codelineno-10-10 name=__codelineno-10-10 href=#__codelineno-10-10></a><span class=n>train_data</span> <span class=o>=</span> <span class=n>dataset</span>  <span class=c1># Small dataset, no split</span>
</span></code></pre></div> <p><strong>Fine-Tuning</strong></p> <p>python</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>"distilgpt2"</span><span class=p>)</span>  <span class=c1># Smaller model</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>"distilgpt2"</span><span class=p>)</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a><span class=n>model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>lora_config</span><span class=p>)</span>  <span class=c1># Reuse LoRA config</span>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>SFTTrainer</span><span class=p>(</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>train_data</span><span class=p>,</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a>    <span class=n>args</span><span class=o>=</span><span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-11-11><a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a>        <span class=n>output_dir</span><span class=o>=</span><span class=s2>"./summary_model"</span><span class=p>,</span>
</span><span id=__span-11-12><a id=__codelineno-11-12 name=__codelineno-11-12 href=#__codelineno-11-12></a>        <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-11-13><a id=__codelineno-11-13 name=__codelineno-11-13 href=#__codelineno-11-13></a>        <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-11-14><a id=__codelineno-11-14 name=__codelineno-11-14 href=#__codelineno-11-14></a>        <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-4</span><span class=p>,</span>
</span><span id=__span-11-15><a id=__codelineno-11-15 name=__codelineno-11-15 href=#__codelineno-11-15></a>        <span class=n>fp16</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-11-16><a id=__codelineno-11-16 name=__codelineno-11-16 href=#__codelineno-11-16></a>    <span class=p>),</span>
</span><span id=__span-11-17><a id=__codelineno-11-17 name=__codelineno-11-17 href=#__codelineno-11-17></a>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-11-18><a id=__codelineno-11-18 name=__codelineno-11-18 href=#__codelineno-11-18></a>    <span class=n>dataset_text_field</span><span class=o>=</span><span class=s2>"prompt"</span><span class=p>,</span>
</span><span id=__span-11-19><a id=__codelineno-11-19 name=__codelineno-11-19 href=#__codelineno-11-19></a>    <span class=n>max_seq_length</span><span class=o>=</span><span class=mi>128</span>
</span><span id=__span-11-20><a id=__codelineno-11-20 name=__codelineno-11-20 href=#__codelineno-11-20></a><span class=p>)</span>
</span><span id=__span-11-21><a id=__codelineno-11-21 name=__codelineno-11-21 href=#__codelineno-11-21></a>
</span><span id=__span-11-22><a id=__codelineno-11-22 name=__codelineno-11-22 href=#__codelineno-11-22></a><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-11-23><a id=__codelineno-11-23 name=__codelineno-11-23 href=#__codelineno-11-23></a><span class=n>trainer</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>"./summary_model_final"</span><span class=p>)</span>
</span></code></pre></div> <p><strong>Conclusion</strong></p> <p>Supervised Fine-Tuning is a pivotal technique for adapting pre-trained LLMs to specific tasks, enhancing their performance and utility across various domains. By understanding its process, leveraging efficient techniques like PEFT and TRL, and addressing challenges, developers can unlock the full potential of LLMs, making them indispensable tools for specialized applications. This comprehensive approach ensures both theoretical insight and practical applicability, supported by a wealth of resources and examples.</p> <p><strong>Key Citations</strong></p> <ul> <li><a href=https://www.superannotate.com/blog/llm-fine-tuning>SuperAnnotate Fine-tuning large language models LLMs in 2024</a></li> <li><a href=https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3>Medium Supervised Fine-tuning customizing LLMs</a></li> <li><a href=https://arxiv.org/pdf/2408.13296.pdf>arXiv The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs</a></li> <li><a href=https://huggingface.co/blog/rishiraj/finetune-llms>Hugging Face Fine-Tuning LLMs Supervised Fine-Tuning and Reward Modelling</a></li> <li><a href=https://www.sama.com/blog/supervised-fine-tuning-how-to-choose-the-right-llm>Sama Supervised Fine-Tuning How to choose the right LLM</a></li> <li><a href=https://nebius.com/blog/posts/fine-tuning/supervised-fine-tuning>Nebius What is supervised fine-tuning in LLMs</a></li> <li><a href=https://www.turing.com/resources/finetuning-large-language-models>Turing Fine-Tuning LLMs Overview Methods Best Practices</a></li> <li><a href=https://datasciencedojo.com/blog/fine-tuning-llms/ >Data Science Dojo Fine-tuning LLMs 101</a></li> <li><a href=https://kili-technology.com/large-language-models-llms/the-ultimate-guide-to-fine-tuning-llms-2024>Kili Technology What is LLM Fine-Tuning Everything You Need to Know 2023 Guide</a></li> <li><a href=https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations>GPT-4all Dataset for fine-tuning</a></li> <li><a href=https://github.com/gururise/AlpacaDataCleaned>AlpacaDataCleaned for fine-tuning</a></li> <li><a href=https://huggingface.co/datasets/databricks/databricks-dolly-15k>databricks-dolly-15k for fine-tuning</a></li> </ul></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="March 6, 2025 19:59:31">March 6, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="March 6, 2025 19:59:31">March 6, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../../TheoryBehindFinetuning/SFT/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Theory behind SFT"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Theory behind SFT </div> </div> </a> <a href=../SFT_finetuning_notebook/ class="md-footer__link md-footer__link--next" aria-label="Next: Finetune Any LLM with SFT"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Finetune Any LLM with SFT </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Adithya S Kolavi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"/></svg> </a> <a href=https://x.com/adithya_s_k target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../../assets/javascripts/custom.9e5da760.min.js></script> <!-- Rich Snippets / Structured Data --> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "EducationalOrganization",
    "name": "AI Engineering Academy",
    "url": "https://aiengineering.academy",
    "logo": "https://aiengineering.academy/assets/logo.png",
    "description": "A structured learning platform for AI engineers with clear paths in prompt engineering, RAG, fine-tuning, deployment, and agent development.",
    "sameAs": [
      "https://github.com/adithya-s-k/AI-Engineering.academy",
      "https://x.com/adithya_s_k"
    ],
    "founder": {
      "@type": "Person",
      "name": "Adithya S Kolavi"
    },
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    }
  }
</script> </body> </html>